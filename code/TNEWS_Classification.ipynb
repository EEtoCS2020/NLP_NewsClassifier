{"cells":[{"cell_type":"code","metadata":{"id":"udA1UiqnVKRI","colab_type":"code","outputId":"fe196520-217a-4a62-e41e-5cc9bd71de9d","executionInfo":{"status":"ok","timestamp":1591602768169,"user_tz":-480,"elapsed":3963,"user":{"displayName":"Neal Lin","photoUrl":"","userId":"17485536999194959255"}},"colab":{"base_uri":"https://localhost:8080/","height":323}},"source":["\n","import torch\n","import numpy as np\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.optim import Adam\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import time\n","import sys\n","#Google Colab 路径\n","# sys.path.append('/content/drive/My Drive/pkuss-nlp-TNEWS-Multiclass')\n","# 个人小建议：系统的包放在上面，个人的包放在下面，这样遇到word_dir出问题的时候中间可以直接修改work_dir\n","from utils.tokenizer import Tokenizer\n","from utils.get_emb import *\n","from models.LSTMClassifier import LSTMClassifierNet\n","from models.CNNClassifier import CNNClassifierNet\n","from models.LSTMAttentionClassifier import LSTMAttentionClassifierNet\n","from dataset_readers.Tnews_MultiClassCorp import *\n","from utils.label import *\n","\n","\"\"\"超参数设定\"\"\"\n","# hidden_size = 1024  # 使用RNN变种LSTM单元   LSTM的hidden size\n","# num_layers = 1      # 循环单元/LSTM单元的层数\n","epoch = 10           # 迭代轮次\n","# num_samples = 1000  # 测试语言模型生成句子时的样本数\n","batch_size = 24     # 一批样本的数量\n","seq_length = 70     # 一个样本/序列长度\n","learning_rate = 0.002 # 学习率\n","\n","\n","def load_data(seq_length, label):\n","    print(\"Start to load data.\")\n","    start_time = time.time()\n","    # emb: collections.OrderedDict()顺序字典\n","    ## key:词(str类型), value:词向量(list类型，元素为float)\n","    # dict_length: 字典大小\n","    # emb_size: 词向量的维数\n","    emb, dict_length, emb_size = get_emb()\n","    # 用所有的词(str类型)实例化一个tokenizer\n","    tokenizer = Tokenizer(emb.keys())\n","    # emb_matrix: ID与词向量的对应的矩阵\n","    ## ID: 每种字对应一个ID号，比如“的”1号，“是”2号以此类推\n","    ## 矩阵第一维的坐标就是ID号，ID号这一行的向量即对应的词向量\n","    emb_matrix = get_emb_matrix(emb, tokenizer, dict_length, emb_size)\n","\n","    # 生成ChnSentiCorp_Clf类的实例\n","    ## 类的构造函数已经将数据切分成训练数据和测试数据\n","    data_loader = Tnews_ChnCorp_Clf(label)\n","    # 获取训练数据\n","    ## list类型，以data_example类的实例为元素\n","    ## data_example类包含2个属性：text，str类型；label，str类型\n","    train_examples = data_loader.get_train_examples()\n","    # 获取验证数据\n","    ## 同train_examples\n","    dev_examples = data_loader.get_dev_examples()\n","\n","    def generate_dataloader(examples, tokenizer, seq_length):\n","        \"\"\"\n","        生成数据加载器\n","        :param examples: list类型，以data_example类的实例为元素。\n","                        data_example类包含2个属性：text，str类型；label_id，int类型\n","        :param tokenizer:\n","        :param seq_length: 一个样本/序列长度\n","        :return: dataloader，迭代器类型；\n","        \"\"\"\n","        features = multi_convert_example_to_feature(examples, tokenizer, seq_length)\n","        # ids，tensor类型(转自list类型)\n","        # 每个元素代表一个样本的text文本对应的ID号序列，list类型\n","        # 一个字对应一个ID号\n","        ids = torch.tensor([f.ids for f in features], dtype=torch.long)\n","        # labels，tensor类型(转自list类型)\n","        # 每个元素是一个样本对应的标签ID号\n","        label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n","\n","        dataset = TensorDataset(ids, label_ids)\n","        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","        return dataloader\n","           \n","    train_dataloader = generate_dataloader(train_examples, tokenizer, seq_length)\n","    dev_dataloader = generate_dataloader(dev_examples, tokenizer, seq_length)\n","\n","    end_time = time.time()\n","    print(\"Data loading finishes. Time span: {:.2f}s\".format(end_time - start_time))\n","\n","    return emb_matrix, train_dataloader, dev_dataloader, tokenizer\n","\n","\n","def load_model(seq_length, label_len, emb_matrix):\n","    print(\"Start to load model.\")\n","    start_time = time.time()\n","    # TODO: you can choose different model\n","    # model = CNNClassifierNet(seq_length, label_num, emb_matrix)\n","    # model = LSTMClassifierNet(seq_length, label_num, emb_matrix, bidirectional=True)\n","    model = LSTMAttentionClassifierNet(seq_length, label_len, emb_matrix, hidden_dims=200,bidirectional=True,num_layers=3)\n","    if torch.cuda.is_available():\n","        model.to(torch.device('cuda'))\n","    optimizer = Adam(model.parameters(), lr=learning_rate)\n","\n","    end_time = time.time()\n","    print(\"Model loading finishes. Time span: {:.2f}s\".format(end_time - start_time))\n","\n","    return model, optimizer\n","\n","\n","def train(model, optimizer, train_dataloader, dev_dataloader, epoch=5):\n","    print(\"Start to train the model.\")\n","    start_time = time.time()\n","    for i in range(epoch):\n","        model.train()\n","        total_loss = []\n","        for ids, label_ids in train_dataloader:\n","            if torch.cuda.is_available():\n","                ids = ids.to(torch.device('cuda'))\n","                label_ids = label_ids.to(torch.device('cuda'))\n","            optimizer.zero_grad()\n","            # 为模型传入字符的ID列表和标签ID列表\n","            # 当传入标签，模型认为此时为训练状态，则返回损失值\n","            loss = model(ids, label_ids)\n","            # loss为标量，0维\n","            # 使用loss.item()从标量中获取Python数字\n","            total_loss.append(loss.item())\n","            loss.backward()\n","            optimizer.step()\n","        print(\"epoch: %d, loss: %.6f\" % (i + 1, sum(total_loss) / len(total_loss)))\n","        #Google Colab 路径\n","        # torch.save(model.state_dict(), '/content/drive/My Drive/pkuss-nlp-TNEWS-Multiclass/models/TNews_Classifier_Model.bin')\n","        model.eval()\n","        total_labels = []\n","        total_pred = []\n","        for ids, label_ids in dev_dataloader:\n","            if torch.cuda.is_available():\n","                ids = ids.to(torch.device('cuda'))\n","            # logits即表示分类模型产生的一个预测结果，一般接着输入Softmax\n","            # logits (batch_size, label_num)\n","            # the logits vector of raw (non-normalized) predictions that a classification model generates,\n","            # which is ordinarily then passed to a normalization function.\n","            # If the model is solving a multi-class classification problem,\n","            # logits typically become an input to the softmax function.\n","            # The softmax function then generates a vector of (normalized) probabilities with one value for each possible class.\n","            logits = model(ids)\n","            # 转成numpy\n","            logits = logits.detach().cpu().numpy()\n","            # 在倒数第一个维度上求最大值的下标\n","            # m = [\n","            #       [1, 2, 3],\n","            #       [4, 8, 6]\n","            #      ]\n","            # logits = np.argmax(m, axis=-1)\n","            # print(logits)  # [2 1]\n","            # 即求每个batch上label概率值最大的下标\n","            logits = np.argmax(logits, axis=-1)\n","            # 将numpy转换成list类型\n","            logits = logits.tolist()\n","            # 追加入total_pred列表后\n","            total_pred.extend(logits)\n","            label_ids = label_ids.numpy().tolist()\n","            total_labels.extend(label_ids)\n","        # eval_p = precision_score(total_labels, total_pred)\n","        # eval_r = recall_score(total_labels, total_pred)\n","\n","        # The F1 Score is the 2*((precision*recall)/(precision+recall))\n","        # F1 score conveys the balance between the precision and the recall.\n","        # 多分类问题，应设置average参数\n","        eval_f1 = f1_score(total_labels, total_pred, average='micro')\n","        print(\"eval_f1: %.2f%%\" % (eval_f1 * 100))\n","\n","    end_time = time.time()\n","    print(\"Model training finishes. Time span: {:.2f}s\".format(end_time - start_time))\n","\n","def tensor_to_label(logits):\n","    \"\"\"\n","\n","    :param logits: 预测结果的概率分布，(1, label_num)；因为输入仅一段话，故第一维度是1\n","    :return:\n","    \"\"\"\n","    # detach()就是截断反向传播的梯度流\n","    # 将logits转化成numpy()\n","    # logits (batch_size, label_num)\n","    logits = logits.detach().cpu().numpy()\n","    # 选出每个预测的分布中，概率最大值的下标\n","    pred = np.argmax(logits, axis=-1)\n","    return label.id_to_desc(pred[0])\n","\n","\n","def test(model, tokenizer, seq_length):\n","    \"\"\"\n","    测试模型：判断输入字符串的文本类型\n","    \"\"\"\n","    print(\"Start to test the model.\")\n","    while True:\n","        s = input()\n","        if s == 'quit':\n","            break\n","        s = [data_example(s, 0)]\n","        s_features = multi_convert_example_to_feature(s, tokenizer, seq_length)\n","        ids = torch.tensor([f.ids for f in s_features], dtype=torch.long)\n","        with torch.no_grad():\n","            if torch.cuda.is_available():\n","                ids = ids.to(torch.device('cuda'))\n","\n","            res = tensor_to_label(model(ids))\n","            print(res)\n","        print(\"Stop testing.\")\n","\n","\n","\n","label = Label()\n","label_num = label.cal_label_num()\n","print(\"label_num:\", label_num)\n","emb_matrix, train_dataloader, dev_dataloader, tokenizer = load_data(seq_length, label)\n","\n","model, optimizer = load_model(seq_length, label_num, emb_matrix)\n","#Google Colab 路径\n","#model.load_state_dict(torch.load('/content/drive/My Drive/pkuss-nlp-TNEWS-Multiclass/models/TNews_Classifier_Model.bin'))\n","\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["label_num: 15\n","Start to load data.\n","dict_length:  9109\n","emb_size:  300\n","UNK 0\n","， 1\n","的 2\n","。 3\n","、 4\n","和 5\n","在 6\n","年 7\n","“ 8\n","了 9\n","Data loading finishes. Time span: 2.69s\n","Start to load model.\n","self.emb_size:  300\n","Model loading finishes. Time span: 0.03s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nlo-2xqkCz6t","colab_type":"code","outputId":"232e4a5e-47db-48c5-c9f6-22df44753dd2","executionInfo":{"status":"ok","timestamp":1591603596961,"user_tz":-480,"elapsed":832715,"user":{"displayName":"Neal Lin","photoUrl":"","userId":"17485536999194959255"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"source":["train(model, optimizer, train_dataloader, dev_dataloader, epoch=epoch)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Start to train the model.\n","epoch: 1, loss: 2.115184\n","eval_f1: 49.97%\n","epoch: 2, loss: 1.444651\n","eval_f1: 57.63%\n","epoch: 3, loss: 1.246329\n","eval_f1: 59.08%\n","epoch: 4, loss: 1.106286\n","eval_f1: 60.00%\n","epoch: 5, loss: 0.986658\n","eval_f1: 60.46%\n","epoch: 6, loss: 0.875246\n","eval_f1: 60.73%\n","epoch: 7, loss: 0.772575\n","eval_f1: 61.51%\n","epoch: 8, loss: 0.693900\n","eval_f1: 60.80%\n","epoch: 9, loss: 0.629211\n","eval_f1: 60.78%\n","epoch: 10, loss: 0.576872\n","eval_f1: 60.59%\n","Model training finishes. Time span: 828.74s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RtT-itjWkyH-","colab_type":"code","outputId":"9c39742d-593a-4b43-b112-3226f90ee8b4","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1591604523009,"user_tz":-480,"elapsed":165312,"user":{"displayName":"Neal Lin","photoUrl":"","userId":"17485536999194959255"}}},"source":["# 使用模型\n","test(model, tokenizer, seq_length)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Start to test the model.\n","发改委:海南自贸港不会冲击香港\n","news_finance\n","Stop testing.\n","作弊被抓后坠亡大学生家属发声\n","news_edu\n","Stop testing.\n","小布什不支持特朗普连任总统\n","news_world\n","Stop testing.\n","高以翔最后一部剧\n","news_entertainment\n","Stop testing.\n","quit\n"],"name":"stdout"}]}],"metadata":{"colab":{"name":"TNEWS_Classification.ipynb","provenance":[],"mount_file_id":"1WehNV9TS08_nVbg1cquc7DphafdFfM-F","authorship_tag":"ABX9TyPWGAkmJW7qsKy0V4m+CVrw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}